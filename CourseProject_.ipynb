{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr0TzB_XD4Nv"
      },
      "source": [
        "# Data Science Programming Languages- DSAI 1303 \n",
        "## Course Project: Sentiment Analysis of Twitter Data\n",
        "\n",
        "Twitter has emerged as a fundamentally new instrument to obtain social measurements. For example, researchers have shown that the \"mood\" of communication on twitter can be used to predict the stock market. \n",
        "\n",
        "In this programming project you will:\n",
        "\n",
        "* Load and prepare a collected set of twitter data for analysis\n",
        "* You will estimate the sentiment associated with individual tweets\n",
        "* You will estimate the sentiment of a particular term\n",
        "\n",
        "Please keep in mind the following points:\n",
        "* This assignment is open-ended in several ways. You will need to make some decisions about how to best solve each of the problems mentioned above.\n",
        "* **It is absolutely fine to discuss your solutions with your classmates but you are not allowed to share code.**\n",
        "* **Each student must submit their own solution via Google Classroom.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMJVhf4qD4Ny"
      },
      "source": [
        "## Formatting of Twitter Data\n",
        "\n",
        "Strings in the twitter data prefixed with the letter \"u\" are unicode strings. For example: `u\"This is a string\"`.\n",
        "\n",
        "Unicode is a standard for representing a mach larger variety of characters beyond the roma alphabet (greek, russian, mathematical symbols, logograms from non-phonetic writing systems, etc.).\n",
        "\n",
        "In most circumstances, you will be able to use a unicode object just like a string.\n",
        "\n",
        "If you encounter an error involving printing unicode, you can use the [encode](https://docs.python.org/3/library/stdtypes.html#str.encode) method to properly print the international characters. You can find more information about UNICODE and Python 3 [here](https://docs.python.org/3/howto/unicode.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFHYEWo1D4Nz"
      },
      "source": [
        "# Question 1: Loading and Cleaning Twitter Data [20 points]\n",
        "\n",
        "In this first part, you will neeed to load a sample of tweets in memory and prepare them for analysis. The tweets are stored in the file `tweets.json`. This file follows the *JSON* format. JSON stands for JavaScript Object Notation. It is a simple format for representing nested structres of data --- lists of lists of dictionaries of lists of ... you get the idea.\n",
        "\n",
        "Each line in of `tweets.json` represents a message. It is straightforward to convert a JSON string into a Python data structure; there is a library to do so called `json`. Below we will show you how to load the data and how to parse the first line in the `tweets.json` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JB9jOMnD4Nz"
      },
      "source": [
        "Each entry in `tweets.json`, i.e., each `tweet`, corresponds to a dictionary that contains lots of information about the tweet, the user, the activity related to the tweet (i.e., if it was retweeted or not), the timestamp of the tweet, entities mentioned in the tweet, hashtags used, etc.\n",
        "\n",
        "You can treat the `tweet` variable from above as a dicitonary and use the `.keys()` command to see the fields associated with the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPsGbJU0D4N0"
      },
      "source": [
        "We can select any of the aforemented values of Variable `tweet` by treating it as a dictionary. For example let's select the `text` body of the tweet, the time it was `created_at`, and the `hashtags` it contains.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcp1PioID4N0"
      },
      "source": [
        "As you can see this tweet contains no hashtags. The body of the tweet contains several information that is not necesary for our sentiment analysis task. For example, it contains a comma, a reference to a twitter user and a link to an external website. \n",
        "\n",
        "Since this information is not necessary we can remove it. In other words we need to clean our input in order to prepare it for analysis. Next, we show you some basic cleaning operations using **regular expressions**. You can find more information on regular expressions [here](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp2-L4m7D4N1",
        "outputId": "f54aa385-c5f2-4351-a3ed-e3f3c262ed1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean tweet body:\n",
            "0      user hello rose ! love the colors and her flo...\n",
            "1      special delivery of flowers for user !!   hi ...\n",
            "2      i was in the group on the left with the blue ...\n",
            "3      if youre going to walk to the coffee shop hav...\n",
            "4                           he cant be this funny more \n",
            "5      this is so funny have to be petty to crop the...\n",
            "6                                  madam what is  funny\n",
            "7      its funny to see how they stop talking about ...\n",
            "8      user an a funny microphone  like regularly sc...\n",
            "9      it is so fucking funny i was watching it infr...\n",
            "10     my lil pussc be saying wtf she said no funny ...\n",
            "11     user omg hey we never really talk but i think...\n",
            "12     isnt it funny how day by day nothing changes ...\n",
            "13     user it would be a pretty funny way to integr...\n",
            "14            user hope he never sees one if his films \n",
            "15     just gooosebumps merely thinking of all the d...\n",
            "16          user user excellent music , beautiful city \n",
            "17     i really want to clear my hair but i have fil...\n",
            "18                              do you like the film ? \n",
            "19                               user thank you my love\n",
            "20                        user stay healthy we love you\n",
            "21    user user yeah crying with laughter that she s...\n",
            "22    user i really hate how my grades look this sem...\n",
            "23    i am scared to step out the car and look at th...\n",
            "24       wishing all buddhists a very happy wesak day !\n",
            "25    i am happy for my seniors next year , i am a p...\n",
            "26       thank you ! i love your art ! , art , love art\n",
            "27                             user i hate it here link\n",
            "28     i hear laughter in the rain ! ! ! ! thank you...\n",
            "29     dakkani language is so beautiful food die hoo...\n",
            "30                 makes the worst laptops in existence\n",
            "31               make better and more reliable products\n",
            "32     great set of photographs and colours of the f...\n",
            "33     one of my favorite flowers are in bloom in my...\n",
            "34     i love irises ! my favourite two weeks of the...\n",
            "35     i hate to say it but it really never fails th...\n",
            "36     it is a good movie ! one of the best in terms...\n",
            "37     was looking for a feel good film to watch ton...\n",
            "Name: Text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Basic steps for cleaning process.\n",
        "import re\n",
        "import numpy\n",
        "import pandas as pd\n",
        "Data = pd.read_excel(r\"C:\\Users\\pc\\Downloads\\مشروع البرمجة\\DATA.xlsx\")\n",
        "def Twitter_filter(x):\n",
        "    Del_User = re.sub('(@\\w\\w+)','User',x)\n",
        "    Del_URL = re.sub('((https?)://(www)?\\.?(\\w+)\\.(\\w+):?(\\d+)?/?(.+))',\"Link\",Del_User)\n",
        "    return Del_URL\n",
        "F_Data =Data.Text.apply(Twitter_filter)\n",
        "data_filter = Data.filter([\"Topic\"])\n",
        "Data = pd.concat([F_Data, data_filter], axis = 1)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u'\\U00010000-\\U0010ffff'\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                u\"\\ufe0f\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return (emoji_pattern.sub(r'', string)).lower()\n",
        "Body =Data['Text'].apply(remove_emoji) \n",
        "print('Clean tweet body:')\n",
        "print(Body)\n",
        "file = open(\"clean_tweets.txt\" , 'w')\n",
        "file.write(str(Body))\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTqXIFLTD4N3"
      },
      "source": [
        "We are providing you with a Python script named `preprocess.py`. The script `preprocess.py` accepts one argument on the command line: a JSON file with tweets (i.e., `tweets.json`). You can run the program like this:\n",
        "\n",
        "`$ python3 preprocess.py tweets.json`\n",
        "\n",
        "**There are some parts specified in this script that you need to implement**. The goal of this script is to clean all the tweets in `tweets.json`. Running `preprocess.py` will generate an output file named `clean_tweets.txt` containing **one string per line** containing a clean tweet. The order of the clean tweets in your output file should follow the order of the lines in the original `tweets.json`. Basically, the first line in `clean_tweets.txt` should correspond to the first raw tweet in `tweets.json`, the second line should correspond to the second tweet, and so on. If you perform any sorting or you put the processed data in a dictionary the order will not be preserved. Once again: **The n-th line of `clean_tweets.txt` (the file you will submit) should be a string that represent the clean version of the n-the line in the `tweets.json` (the input file).**\n",
        "\n",
        "You must provide a line for **every** tweet. If the clean tweet is the empty string then just provide a line with the empty string.\n",
        "\n",
        "***What to turn in: The file `clean_tweets.txt` output by `preprocess.py` after you have implemented the missing parts in `preprocess.py`.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEh5mWggD4N3"
      },
      "source": [
        "# Question 2: Derive the sentiment of each tweet [40 points]\n",
        "\n",
        "For this part, you will compute the sentiment of each clean tweet in `clean_tweets.txt` based on the sentiment scores of the terms in the tweet. The sentiment of a tweet is equivalent to the sum of the sentiment scores for each term in the clean tweet.\n",
        "\n",
        "You are provided with a skeleton file `tweet_sentiment.py` which accepts two arguments on the command line: a *sentiment file* and a tweet file like the one you generated in Question 1. You can run the skeleton program like this:\n",
        "\n",
        "`$ python3 tweet_sentiment.py AFINN-111.txt clean_tweets.txt`\n",
        "\n",
        "The file `AFINN-111.txt` contains a list of pre-computed sentiment scores. Each line in the file contains a word or phrase phollowed by a sentiment score. Each word or phrase that is found in a tweet but not found in `AFINN-111.txt` should be given a sentiment score of 0. See the file `AFINN-README.txt` for more information.\n",
        "\n",
        "To use the data in the `AFINN-111.txt` file, you may find it useful to build a dictionary. Note that the `AFINN-111.txt` file format is tab-delimited, meaning that the term and the score are separated by a tab character. A tab character corresponds to the string \"\\t\". The following snipped of code may be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtdyWg1RD4N4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "afinnfile_name = open('AFINN-111.txt')\n",
        "afinnfile = open('AFINN-111.txt', 'r')\n",
        "scores = {} # initialize an empty dictionary\n",
        "for line in afinnfile:\n",
        "    term, score = line.split(\"\\t\") # The file is tab-delimited and \"\\t\" means tab character\n",
        "    scores[term] = int(score) # Conver the score to an integer. It was parsed as a string.\n",
        "afinnfile.close()\n",
        "#print(scores.items( )) # Print every (term, score) pair in the dictionary\n",
        "Tweet_list = []\n",
        "not_important = ['to','is','or','since','at','under','my','we','i','at','up','you','it','on','in','from','about','with','for','of','by','What','when','where','which'\n",
        "                'who','whom','why','how','how many']\n",
        "for tweet in Body :\n",
        "    word_list = tweet.split(' ')\n",
        "    Tweet = []\n",
        "    for word in word_list:\n",
        "        word = word.rstrip('.,?!:' '')\n",
        "        Tweet.append(word)\n",
        "    Tweet_list.append(Tweet)\n",
        "val_exist = {}\n",
        "val_not_exist = {}\n",
        "for tweets in range(len(Tweet_list)) :\n",
        "    Sum_Word =0\n",
        "    for Word in Tweet_list[tweets]:\n",
        "        if Word in list(scores.keys()):\n",
        "            Sum_Word = Sum_Word + scores[Word]\n",
        "            val_exist[tweets] = Sum_Word\n",
        "            #print(r)\n",
        "        if Word not in list(scores.keys()) and (Word != '') and ( Word not in not_important):\n",
        "            val_not_exist[tweets] = Word\n",
        "    \n",
        "Twite_Sum = pd.Series(val_exist)  \n",
        "file = open(\"sentiment.txt\" , 'w')\n",
        "file.write(str(Twite_Sum))\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM6f5c--D4N4"
      },
      "source": [
        "Your script should output a file named `sentiment.txt` containing the sentiment of each tweet in the file `clean_tweets.txt`, one numeric sentiment score per line. The first score should correspond to the first tweet, the second score should correspond to the second tweet, and so on. In other words, ** the n-th line of the file you submit should contain only a single number that represents teh score of the n-th tweet in the input file.**\n",
        "\n",
        "After you have implemented everything the first 10 lines of the generated output of your script should be exactly the same as the next lines:\n",
        "\n",
        "```\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "2\n",
        "-4\n",
        "0\n",
        "0\n",
        "```\n",
        "\n",
        "***What to turn in: The file `sentiment.txt` after you have verified that it returns the correct answers***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6zaVe1xD4N5"
      },
      "source": [
        "# Question 3: Derive the sentiment of new terms [40 points]\n",
        "\n",
        "In this part you will create a script that computes the sentiment for terms that **do not** appear in the file `AFINN-111.txt`.\n",
        "\n",
        "You can think about this problem as follows: We know we can use the sentiment-carrying words in `AFINN-111.txt` to deduce the overall sentiment of a tweet. Once you deduce the sentiment of a tweet, you can work backwards to deduce the sentiment of the non-sentiment carrying words that *do not appear* in `AFINN-111.txt`. For example, if the word *football* always appears in proximity with positive words like *great* and *fun*, then we can deduce that the term *football* itself carried a positive sentiment.\n",
        "\n",
        "You are provided with a skeleton file `term_sentiment.py` which accepts the same two arguments as `tweet_sentiment.py` and can be executed using the following command:\n",
        "\n",
        "`$ python3 term_sentiment.py AFINN-111.txt clean_tweets.txt`\n",
        "\n",
        "Your script should print its output to stdout. Each line of the output should contain a term, followed by a space, followed by a sentiment. That is, each line should be in the format <term:string> <sentiment:float>. For example if you have the pair (\"foo\", 54.2) in Python, it should appear in the output as: `foo 54.2`.\n",
        "\n",
        "*The order of your output does not matter.*\n",
        "\n",
        "***What to turn in: The file `term_sentiment.py` after you have implemented the missing parts.***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwBGt4RYD4N5"
      },
      "outputs": [],
      "source": [
        "put_values = {}\n",
        "for tweet_sum in val_exist.values():\n",
        "    if tweet_sum > 0 :\n",
        "        for value in val_not_exist.values() :\n",
        "            if len(value) > 3 :\n",
        "                put_values[value]= 2\n",
        "            if len(value) > 5:\n",
        "                put_values[value] = 4\n",
        "    if tweet_sum < 0 :\n",
        "        for value in val_not_exist.values():\n",
        "            if len(value) >= 3 :\n",
        "                put_values[value]= -2\n",
        "            if len(value) >= 5 :\n",
        "                put_values[value] = -3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOjtahOKD4N6",
        "outputId": "3f12d147-3b33-434a-8c23-3de8605010cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 10, 1: 12, 2: -2, 3: 2, 4: 6, 5: 4, 6: 6, 7: 5, 8: 10, 9: -6, 10: -5, 11: 8, 12: 4, 13: 9, 14: 6, 15: 9, 16: 12, 17: 8, 18: 4, 19: 7, 20: 9, 21: 5, 22: 3, 23: -8, 24: 2, 25: 7, 26: 2, 27: 1, 28: 6, 29: 2, 30: 1, 31: 8, 32: 9, 33: 8, 34: 9, 35: -3, 36: 8, 37: 10}\n"
          ]
        }
      ],
      "source": [
        "for key in put_values.keys() :\n",
        "    scores[key] = put_values[key]\n",
        "for tweets in range(len(Tweet_list)) :\n",
        "    Sum_Word =0\n",
        "    for Word in Tweet_list[tweets]:\n",
        "        if Word in list(scores.keys()):\n",
        "            Sum_Word = Sum_Word + scores[Word]\n",
        "            val_exist[tweets] = Sum_Word\n",
        "print(val_exist)\n",
        "file = open(\"term_sentiment.py\" , 'w')\n",
        "file.write(str(val_exist))\n",
        "file.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}